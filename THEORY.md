## Theory of Neural Networks

Brief interpretations of various subjects in the space of Neural Networks

## Basics

Below shows various questions I've thought of as I'm trying to formulate my theorical understanding of the subjects.

### Fundamentals

> What is it?

> How do we use them?

> How do we create them?

> Whats the consequence (from perspective X) ?

### Layers

#### What is a input layers, and what do we place here?

#### What is a hidden layer, and what types do we have?

#### What is a output layer, and how do we design this to meet our goal?

#### Whats the deal with linear layers?

#### Whats the deal with convolutional layers?

#### What happens when we use multiple-hidden layers?

#### Why do we sometimes drop certain layers?

#### What the deal with fully-connected layers?

#### Whats the deal with things between linear layers?

#### Sometimes linear layers have added bias, why is that?

#### Whats the deal with activation layers?

#### Does the combinations of linear operations and activation layers matter?

#### There seems to be a certain combation of layers, most frequently used - why is that?

### Pass functions

#### Whats a forward pass, and what is imporantant to think of when designing this function?

#### Whats a backward pass (backprop), and what is imporantant to think of when designing this function?

### Activation functions

#### Top 3 most-commenly used

### Regulaztion

### Optimizers

#### Top 3 most-commenly used, and why they work

### Loss functions

#### Top 3 most-commenly used, and why they work

### Special events

#### Drop-out layers?

#### Learning rate?

#### Changig learning rate progressivly (momentum) ?

#### Resetting gradient

#### Underfitting

#### Overfitting

#### Measuring performance

#### Performance indicators?

#### How do we use this neaural net?

#### Inference: GPU vs CPU: when to use what, and what do we consider?
